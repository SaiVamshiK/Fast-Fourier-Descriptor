{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0450c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65aee313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d05c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 224\n",
    "H = 224\n",
    "label_to_class = {\n",
    "    'MildDemented': 0,\n",
    "    'ModerateDemented': 1,\n",
    "    'NonDemented': 2,\n",
    "    'VeryMildDemented':3\n",
    "}\n",
    "class_to_label = {v: k for k, v in label_to_class.items()}\n",
    "n_classes = len(label_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac08a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(dir_name='C:\\\\Users\\\\User\\\\Desktop\\\\Dataset', label_to_class=label_to_class):\n",
    "    Images = []\n",
    "    Classes = []\n",
    "    for j in ['/train','/test']:\n",
    "        for label_name in os.listdir(dir_name+str(j)):\n",
    "            cls = label_to_class[label_name]\n",
    "            for img_name in os.listdir('/'.join([dir_name+str(j), label_name])):\n",
    "                img = load_img('/'.join([dir_name+str(j), label_name, img_name]), target_size=(W, H))\n",
    "                img = img_to_array(img)\n",
    "                Images.append(img)\n",
    "                Classes.append(cls)\n",
    "    Images = np.array(Images, dtype=np.float32)\n",
    "    Classes = np.array(Classes, dtype=np.float32)\n",
    "    Images, Classes = shuffle(Images, Classes, random_state=0)\n",
    "    return Images, Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6930dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6400, 224, 224, 3), (6400,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Images, Classes = get_images()\n",
    "Images.shape, Classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab965844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5120, 224, 224, 3), (5120,), (1280, 224, 224, 3), (1280,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_train, indices_test = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False)\n",
    "x_train = Images[indices_train]\n",
    "y_train = Classes[indices_train]\n",
    "x_test = Images[indices_test]\n",
    "y_test = Classes[indices_test]\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe34880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5120, 4), (1280, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, n_classes)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "067ea5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, \n",
    "    rotation_range=30,     # rotation_range is a value in degrees (0-180), a range within which to randomly rotate pictures  \n",
    "    width_shift_range=0.1, # width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally          \n",
    "    height_shift_range=0.1,# shifts images vertically(up or down).                 \n",
    "    horizontal_flip=True,  # horizontal_flip is for randomly flipping half of the images horizontally                 \n",
    "    vertical_flip=False,   # vertical is for randomly flipping half of the images vertically                 \n",
    "    shear_range=0.2,       # shear_range is for randomly applying shearing transformations\n",
    "    zoom_range=0.2         # zoom_range is for randomly zooming inside pictures\n",
    ")\n",
    "datagen_test = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09aaab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (5120, 3, 224, 224)\n",
      "5120 train samples\n",
      "1280 test samples\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 224,224\n",
    "num_classes=4\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "train_dataset = x_train.astype('float32')\n",
    "test_dataset = x_test.astype('float32')\n",
    "train_dataset /= 255\n",
    "test_dataset /= 255\n",
    "print('train_dataset shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "train_labels = y_train\n",
    "test_labels = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ad4b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 3, 224, 224)\n",
      "(1280, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "from scipy.fftpack import fftn,fftshift\n",
    "dims=train_dataset.shape\n",
    "ft_train=[]\n",
    "for i in range(dims[0]):\n",
    "    ft_train.append(np.abs(fftshift(fftn(train_dataset[i,:,:]))))\n",
    "\n",
    "dims=test_dataset.shape\n",
    "ft_test=[]\n",
    "for i in range(dims[0]):\n",
    "    ft_test.append(np.abs(fftshift(fftn(test_dataset[i,:,:])))) #Don't forget to take abs!\n",
    "\n",
    "ft_train=np.asarray(ft_train)\n",
    "ft_test=np.asarray(ft_test)\n",
    "print (ft_train.shape)\n",
    "print (ft_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988c4a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Flatten, Dense\n",
    "import keras\n",
    "model = keras.models.Sequential()\n",
    "VGG = VGG16(include_top=False, input_shape=(W,H,3),pooling='avg')    \n",
    "print(VGG.summary())\n",
    "for layer in VGG.layers:\n",
    "    layer.trainable=False\n",
    "model.add(VGG)\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad1b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(512,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(128,activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dense(32,activation='relu',kernel_initializer='he_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aefd35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,047,268\n",
      "Trainable params: 332,580\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(4,activation='softmax'))    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87a5b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31a5b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights \\n   can be loaded later to continue the training from the state saved.\\n   Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold \\n   out validation dataset.\\n   Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a \\n  'patience' number of epochs, the learning rate is reduced.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = '{epoch:02d}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "callbacks = [earlystop, model_checkpoint_callback, reduce_lr]\n",
    "'''\n",
    "   ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights \n",
    "   can be loaded later to continue the training from the state saved.\n",
    "   Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold \n",
    "   out validation dataset.\n",
    "   Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a \n",
    "  'patience' number of epochs, the learning rate is reduced.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e77ddab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff618d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.AUC(name='auc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65ff8a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 3, 224, 224)\n",
      "(5120, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "dims=ft_train.shape\n",
    "print (dims)\n",
    "trainee=ft_train.reshape(5120,224,224,3)\n",
    "dims=test_dataset.shape\n",
    "testee=ft_test.reshape(1280,224,224,3)\n",
    "print (trainee.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742957da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "144/144 [==============================] - 872s 6s/step - loss: 6.2108 - accuracy: 0.7215 - auc: 0.6701 - val_loss: 1.1861 - val_accuracy: 0.7500 - val_auc: 0.6978\n",
      "Epoch 2/40\n",
      "144/144 [==============================] - 841s 6s/step - loss: 1.0467 - accuracy: 0.7625 - auc: 0.7846 - val_loss: 1.0220 - val_accuracy: 0.7627 - val_auc: 0.8159\n",
      "Epoch 3/40\n",
      "144/144 [==============================] - 863s 6s/step - loss: 1.0051 - accuracy: 0.7771 - auc: 0.8057 - val_loss: 0.9920 - val_accuracy: 0.7881 - val_auc: 0.8164\n",
      "Epoch 4/40\n",
      "144/144 [==============================] - 881s 6s/step - loss: 0.9751 - accuracy: 0.7858 - auc: 0.8143 - val_loss: 0.9828 - val_accuracy: 0.7847 - val_auc: 0.8160\n",
      "Epoch 5/40\n",
      "144/144 [==============================] - 936s 7s/step - loss: 0.9623 - accuracy: 0.7888 - auc: 0.8173 - val_loss: 1.0160 - val_accuracy: 0.7686 - val_auc: 0.8173\n",
      "Epoch 6/40\n",
      "144/144 [==============================] - 887s 6s/step - loss: 0.9553 - accuracy: 0.7916 - auc: 0.8198 - val_loss: 0.9640 - val_accuracy: 0.7979 - val_auc: 0.8209\n",
      "Epoch 7/40\n",
      "144/144 [==============================] - 835s 6s/step - loss: 0.9443 - accuracy: 0.7943 - auc: 0.8240 - val_loss: 0.9861 - val_accuracy: 0.7812 - val_auc: 0.8178\n",
      "Epoch 8/40\n",
      "144/144 [==============================] - 848s 6s/step - loss: 0.9475 - accuracy: 0.7917 - auc: 0.8245 - val_loss: 0.9910 - val_accuracy: 0.7803 - val_auc: 0.8194\n",
      "Epoch 9/40\n",
      "144/144 [==============================] - 855s 6s/step - loss: 0.9471 - accuracy: 0.7913 - auc: 0.8243 - val_loss: 0.9548 - val_accuracy: 0.7969 - val_auc: 0.8269\n",
      "Epoch 10/40\n",
      "144/144 [==============================] - 862s 6s/step - loss: 0.9501 - accuracy: 0.7913 - auc: 0.8236 - val_loss: 0.9898 - val_accuracy: 0.7759 - val_auc: 0.8198\n",
      "Epoch 11/40\n",
      "144/144 [==============================] - 865s 6s/step - loss: 1.0153 - accuracy: 0.7848 - auc: 0.8129 - val_loss: 0.9523 - val_accuracy: 0.7969 - val_auc: 0.8289\n",
      "Epoch 12/40\n",
      "144/144 [==============================] - 865s 6s/step - loss: 0.9369 - accuracy: 0.7951 - auc: 0.8287 - val_loss: 0.9550 - val_accuracy: 0.7935 - val_auc: 0.8267\n",
      "Epoch 13/40\n",
      "144/144 [==============================] - 1052s 7s/step - loss: 0.9380 - accuracy: 0.7943 - auc: 0.8292 - val_loss: 0.9517 - val_accuracy: 0.7988 - val_auc: 0.8315\n",
      "Epoch 14/40\n",
      "144/144 [==============================] - 1390s 10s/step - loss: 0.9396 - accuracy: 0.7939 - auc: 0.8290 - val_loss: 0.9578 - val_accuracy: 0.7896 - val_auc: 0.8237\n",
      "Epoch 15/40\n",
      "144/144 [==============================] - 1410s 10s/step - loss: 0.9327 - accuracy: 0.7953 - auc: 0.8306 - val_loss: 0.9388 - val_accuracy: 0.7983 - val_auc: 0.8328\n",
      "Epoch 16/40\n",
      "144/144 [==============================] - 1412s 10s/step - loss: 0.9317 - accuracy: 0.7950 - auc: 0.8315 - val_loss: 0.9399 - val_accuracy: 0.7993 - val_auc: 0.8321\n",
      "Epoch 17/40\n",
      "144/144 [==============================] - 1416s 10s/step - loss: 0.9247 - accuracy: 0.7956 - auc: 0.8337 - val_loss: 0.9403 - val_accuracy: 0.8003 - val_auc: 0.8341\n",
      "Epoch 18/40\n",
      "144/144 [==============================] - 1065s 7s/step - loss: 0.9206 - accuracy: 0.7969 - auc: 0.8346 - val_loss: 0.9399 - val_accuracy: 0.7998 - val_auc: 0.8339\n",
      "Epoch 19/40\n",
      "144/144 [==============================] - 885s 6s/step - loss: 0.9272 - accuracy: 0.7942 - auc: 0.8336 - val_loss: 0.9456 - val_accuracy: 0.7935 - val_auc: 0.8321\n",
      "Epoch 20/40\n",
      "144/144 [==============================] - 889s 6s/step - loss: 0.9314 - accuracy: 0.7935 - auc: 0.8318 - val_loss: 0.9544 - val_accuracy: 0.7915 - val_auc: 0.8331\n",
      "Epoch 21/40\n",
      " 63/144 [============>.................] - ETA: 7:31 - loss: 0.9129 - accuracy: 0.7966 - auc: 0.8391"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=METRICS)\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "history=model.fit(trainee, train_labels, batch_size=32, verbose=1, epochs=40, validation_split = 0.1)\n",
    "\n",
    "\"\"\"\n",
    "history=model.fit(datagen_train.flow(ft_train,train_labels, batch_size=128,shuffle=True), \n",
    "                  epochs=epochs,validation_data=datagen_test.flow(ft_test,test_labels, \n",
    "                  batch_size=32,shuffle=True))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e82491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c81e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602e95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b189f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd86c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf2e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d22be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd859c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc818707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac35e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f6481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd6352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
